{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bitcoin & Twitter\n",
    "\n",
    "Project on Data Mining by Bontenakel Lenny & Bels Senne.  \n",
    " *[Link to our github repository](https://github.com/snenenenenenene/btc-twitter-data-mining)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "## Research Question\n",
    "\n",
    "In this project we will research the correlation between the activity around cryptocurrency on social media and the actual stock price of said cryptocurrency. In this case we will take the examples of social media channel *Twitter* and the cryptocurrency *Bitcoin*.\n",
    "\n",
    "To achieve this we ask ourselves the following question:\n",
    "\n",
    "Is there a correlation between the activity on Twitter regarding Bitcoin and the stock price of Bitcoin? \n",
    "\n",
    "## Method\n",
    "\n",
    "To answer this question we will look and analyze at past data from Kaggle.com and Yahoo Finance. By analyzing this data we can easily formulate the answer to our question.\n",
    "\n",
    "## Technologies\n",
    "\n",
    "Name       | Reason\n",
    "-----------|--------------------------------\n",
    "PySpark    | Loading and processing datasets\n",
    "Matplotlib | Visualising data\n",
    "Pattern    | Analysing sentiments\n",
    "Folium     | Mapping locations on a map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import *\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import rc\n",
    "import re\n",
    "import datetime\n",
    "from typing import List\n",
    "import numpy as np\n",
    "from wordcloud import WordCloud\n",
    "import folium\n",
    "from PIL import Image\n",
    "import yfinance as yf\n",
    "from geopy.geocoders import Nominatim\n",
    "from pattern.en import sentiment\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col, desc, asc, udf, max, struct\n",
    "\n",
    "sc = SparkContext(\"local\").getOrCreate()\n",
    "sqlContext = SQLContext(sc)\n",
    "spark = SparkSession(sc)\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweets\n",
    "\n",
    "We load tweet data from a dataset found on kaggle. A link to this dataset can be found in the README, provided in our *[github repository](https://github.com/snenenenenenene/btc-twitter-data-mining)*.   \n",
    "This dataset contains all tweets with some regard or mention of Bitcoin. This could be a hashtag or simply any mention of Bitcoin within the text of the tweet.\n",
    "\n",
    "The dataset contains following data:\n",
    "\n",
    "\n",
    "Column           | Description\n",
    "-----------------|------------------------------------------------------------------------------------------\n",
    "user_name        | The name of the user, as they’ve defined it.\n",
    "user_location    | The user-defined location for this account’s profile.\n",
    "user_description | The user-defined UTF-8 string describing their account.\n",
    "user_created     | Time and date, when the account was created.\n",
    "user_followers   | The number of followers an account currently has.\n",
    "user_friends     | The number of friends a account currently has.\n",
    "user_favourites  | The number of favorites a account currently has\n",
    "user_verified    | When true, indicates that the user has a verified account\n",
    "date             | UTC time and date when the Tweet was created\n",
    "text             | The actual UTF-8 text of the Tweet\n",
    "hashtags         | All the other hashtags posted in the tweet along with #Bitcoin & #btc\n",
    "source           | Utility used to post the Tweet, Tweets from the Twitter website have a source value - web\n",
    "is_retweet       | Indicates whether this Tweet has been Retweeted by the authenticating user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_schema = StructType([\n",
    "    StructField('user_name', StringType(), True),\n",
    "    StructField('user_location', StringType(), True),\n",
    "    StructField('user_description', StringType(), True),\n",
    "    StructField('user_created', StringType(), True),\n",
    "    StructField('user_followers', FloatType(), True),\n",
    "    StructField('user_friends', FloatType(), True),\n",
    "    StructField('user_favourites', FloatType(), True),\n",
    "    StructField('user_verified', BooleanType(), True),\n",
    "    StructField('date', StringType(), True),\n",
    "    StructField('text', StringType(), True),\n",
    "    StructField('hashtags', StringType(), True),\n",
    "    StructField('source', StringType(), True),\n",
    "    StructField('is_retweet', BooleanType(), True),\n",
    "])\n",
    "\n",
    "tweets_df = spark.read.csv(\n",
    "    \"./data/tweets.csv\",\n",
    "    header=True,\n",
    "    multiLine=True,\n",
    "    unescapedQuoteHandling=\"STOP_AT_CLOSING_QUOTE\",\n",
    "    schema=tweets_schema\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yahoo finance data\n",
    "\n",
    "We also require a dataset with the stock price of Bitcoin over the period between 26-11-2021 and 5-2-2021. For this we will use the Yahoo Finance library. The data we get from this looks like this:\n",
    "\n",
    "\n",
    "Column       | Description\n",
    "-------------|-------------------------------------------------------\n",
    "Open         | Open value of the currency\n",
    "High         | Highest value of currency in the given minute\n",
    "Low          | Lowest value of currency in the given minute\n",
    "Close        | Close value of the currency in the given minute\n",
    "Volume       | Volume of the currency transacted in the given minute.\n",
    "Dividends    | ...\n",
    "Stock Splits | ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "btc_stock = yf.Ticker(\"BTC-USD\")\n",
    "end = datetime.datetime(2021, 11, 26)\n",
    "start = datetime.datetime(2021, 2, 5)\n",
    "\n",
    "btc_stock = btc_stock.history(start=start, end=end)\n",
    "btc_df = spark.createDataFrame(btc_stock)\n",
    "\n",
    "btc_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Quality\n",
    "\n",
    "## Missing values\n",
    "\n",
    "After having loaded our datasets we need to analyze the quality of our dataset. For this we will take a look at the amount of missing values in our dataset.    \n",
    "The locations of users should however be tolerated. Missing data in this column is not due to a low quality dataset, but due to some users not being willing to share their locations. Which should of course be respected and tolerated. Thus, there will be no actions taken to fill in this data.  \n",
    "The red horizontal baseline indicates the total amount of rows present in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "\n",
    "ax.axhline(y=tweets_df.count(), label=\"Total amount of rows\")\n",
    "ax.bar_label(\n",
    "    ax.bar(\n",
    "        tweets_df.columns,\n",
    "        [tweets_df.where(col(l).isNull()).count() for l in tweets_df.columns]\n",
    "    )\n",
    ")\n",
    "\n",
    "ax.set_xlabel(\"Columns\")\n",
    "ax.set_ylabel(\"# Null values\")\n",
    "\n",
    "ax.ticklabel_format(axis='y', style='plain')\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "With the data loaded and checked. We need to perform some operations to make the data usable for our analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Text cleaning\n",
    "\n",
    "Not all text is clean and some of it needs to be cleant for it to be used. Hyperlinks and newline characters for example interfere with the capabilities of PySpark and might cause a crash whenever we try to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    if (isinstance(text, str)):\n",
    "        text = text.replace(\"#\", \"\")\n",
    "        text = re.sub('\\\\n', '', text)\n",
    "        text = re.sub('https:\\/\\/\\S+', '', text)\n",
    "        return text\n",
    "    else:\n",
    "        return \"\"\n",
    "clean_text_udf = udf(lambda x: clean_text(x), StringType())\n",
    "\n",
    "tweets_df = tweets_df.withColumn(\"text\", clean_text_udf(col(\"text\"))).dropna(subset=[\"user_name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixing the hashtags array column \n",
    "Before we can generate the impact score, we need to generate an array of strings. This array represents the hashtags present within the tweet.\n",
    "Since the csv format - which we use to read in the data - does not support arrays in pyspark. We need to fix it after reading it in as a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_hashtags_array(hashtags_arr_string):\n",
    "    try:\n",
    "        closing_bracket = hashtags_arr_string.index(']', 1)\n",
    "        subject = hashtags_arr_string[1 :closing_bracket]\n",
    "\n",
    "        result = subject.split(', ') if closing_bracket > 1 else []\n",
    "        result = ' '.join(result).replace(\"'\", \"\").split()\n",
    "\n",
    "        return result\n",
    "        \n",
    "    except ValueError:\n",
    "        return []\n",
    "    \n",
    "fix_hashtags_array_udf = udf(lambda x: fix_hashtags_array(x), ArrayType(StringType()))\n",
    "\n",
    "tweets_df = tweets_df.fillna(\"[]\", subset=\"hashtags\").withColumn(\"hashtags\", fix_hashtags_array_udf(col(\"hashtags\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Generating Date Dataframe\n",
    "\n",
    "At this moment we are lacking the column in our Yahoo Finance dataset containing the dates of every Bitcoin stock price. However, we do know the first and last date of the dataset - namely 5-2-2021 and 26-11-2021 - as well as the fact that this data registers new entries at regular intervals. With this knowledge we could generate a date dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "date_df = tweets_df.withColumn(\"date\", F.to_date(F.col(\"date\")))\n",
    "date_df = date_df.groupby(\"date\").count().dropna().sort(asc(\"date\")).filter(\n",
    "    (date_df.date > datetime.datetime(2020, 3, 20)) & (date_df.date < datetime.datetime.today()))\n",
    "\n",
    "counts_df = date_df\n",
    "\n",
    "def _get_next_dates(start_date: datetime.date, diff: int) -> List[datetime.date]:\n",
    "    return [start_date + datetime.timedelta(days=days) for days in range(1, diff)]\n",
    "\n",
    "def _get_fill_dates_df(df: DataFrame, date_column: str, group_columns: List[str], fill_column: str) -> DataFrame:\n",
    "    get_next_dates_udf = udf(_get_next_dates, ArrayType(DateType()))\n",
    "\n",
    "    window = Window.orderBy(*group_columns, date_column)\n",
    "\n",
    "    return df.withColumn(\"_diff\", F.datediff(F.lead(date_column, 1).over(window), date_column)).filter(col(\"_diff\") > 1).withColumn(\"_next_dates\", get_next_dates_udf(date_column, \"_diff\")).withColumn(fill_column, F.lit('')).withColumn(date_column, F.explode(\"_next_dates\")).drop(\"_diff\", \"_next_dates\")\n",
    "\n",
    "fill_df = _get_fill_dates_df(date_df, \"date\",[], \"count\")\n",
    "date_df = date_df.union(fill_df).sort(asc(col(\"date\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Data Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most popular users\n",
    "\n",
    "We will start separating data from the main dataframe, to create a new dataframe showing data about the used accounts.\n",
    "This way we can easily show what accounts are most followed and loved.\n",
    "However, user accounts are constantly changing. The amounts of followers, friends and favourites an accounts has rarely remains the same for long.\n",
    "These values rise and fall, therefore it would not be wise to simply select the instance with the max amount of followers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accounts_df = tweets_df.groupBy('user_name').max('user_followers').withColumnRenamed('max(user_followers)', 'user_followers').sort(desc('user_followers'))\n",
    "\n",
    "x_rows = accounts_df.select('user_name').collect()\n",
    "y_rows = accounts_df.select('user_followers').collect()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "\n",
    "n = 20\n",
    "ax.barh(\n",
    "    [x.user_name for x in x_rows[:n]],\n",
    "    [y.user_followers for y in y_rows[:n]],\n",
    "    color='green',\n",
    "    label='tweets/ user'\n",
    ")\n",
    "\n",
    "ax.ticklabel_format(axis='x', style='plain')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Most active user\n",
    "\n",
    "We will also look at the most active user in the dataset and see how much impact this user has on our question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "user_volume = tweets_df.groupby(\"user_name\").count().withColumnRenamed(\"count\", \"user_count\").sort(desc(\"user_count\"))\n",
    "\n",
    "n = 10\n",
    "x_rows = user_volume.limit(n).select(\"user_name\").collect()\n",
    "y_rows = user_volume.limit(n).select(\"user_count\").collect()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "\n",
    "ax.bar_label(\n",
    "    ax.barh([x.user_name for x in x_rows],\n",
    "           [y.user_count for y in y_rows],\n",
    "           label='tweets/ user')\n",
    ")\n",
    "\n",
    "ax.set_xlabel('# tweets')\n",
    "ax.set_ylabel('username')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Locations of users\n",
    "\n",
    "We're also interested in the locations of users. This does not really aid us in answering our question. But nevertheless it's fun to look at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_location = tweets_df.groupBy('user_location').count().sort(col(\"count\").desc()).show()\n",
    "geolocator = Nominatim(user_agent=\"example\")\n",
    "location_df = tweets_df.groupBy('user_location').count().filter(\"count >= 500\").where(\n",
    "    \"user_location not in ('Decentralized', 'Moon', '🇦🇺', 'Everywhere', 'Road Warrior', 'Mars', 'Cloud Engineer', 'Planet Earth', 'Earth', 'Blockchain', 'The Blockchain')\").sort(\n",
    "    col(\"count\").desc()).dropna().collect()\n",
    "\n",
    "\n",
    "def coords(location_string):\n",
    "    try:\n",
    "        location_obj = geolocator.geocode(location_string).raw\n",
    "        return (location_obj['lat'], location_obj['lon'])\n",
    "    except:\n",
    "        return (20, 20)\n",
    "\n",
    "\n",
    "locations = list(map(lambda r: [r['user_location'], r['count'], coords(r['user_location'])], location_df))\n",
    "map_tweets = folium.Map(location=[51,10], zoom_start=2)\n",
    "\n",
    "for location_name, count, location_coords in locations:\n",
    "    folium.Circle(location=location_coords,\n",
    "                  popup=f\"{location_name}: {count}\",\n",
    "                  radius=count * 50,\n",
    "                  color=\"crimson\",\n",
    "                  fill_color=\"crimson\",\n",
    "                  tooltip=count).add_to(map_tweets)\n",
    "map_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation between amount of tweets and Bitcoin stock price\n",
    "\n",
    "It is then time to take a first poke at our research question. It may be a fairly naive and brute force way to do it, but it provides a decent initial insight in the answer to our research question. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X FOR BTC VOLUME\n",
    "dates = date_df.select(\"date\").collect()\n",
    "x = list(map(lambda r: (r['date']), dates))\n",
    "\n",
    "# X FOR COUNTS\n",
    "counts_dates = counts_df.select(\"date\").collect()\n",
    "counts_x = list(map(lambda r: (r['date']), counts_dates))\n",
    "\n",
    "# Y/COUNT OF TWEETS\n",
    "y_rows = counts_df.select(\"count\").collect()\n",
    "tweets_y = list(map(lambda r: r['count'], y_rows))\n",
    "\n",
    "# Y FOR BTC VOLUME\n",
    "y_rows = btc_df.select(\"Open\").collect()\n",
    "btc_y = list(map(lambda r: float(r['Open']), y_rows))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "ax.plot(counts_dates, tweets_y, color='blue', label='Tweet Volume')\n",
    "# ax.set_yscale('log')\n",
    "ax.tick_params(axis='y')\n",
    "\n",
    "ax2 = ax.twinx()\n",
    "ax2.plot(x,btc_y, color='red', label='BTC Value')\n",
    "# ax2.set_yscale('log')\n",
    "ax2.tick_params(axis='y')\n",
    "\n",
    "lines, labels = ax.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax2.legend(lines + lines2 , labels + labels2, loc=0)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis\n",
    "\n",
    "Of course, not every tweet is the same. It would be naive to conclude our research here. As every tweet carries a different sentiment. There are tweets which speak poorly of Bitcoin and others which praise Bitcoin as our ticket to the future. There are also the occasional bots which are tasked with spamming Twitter with the latest news and data. And these of course carry no sentiment at all.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_text = tweets_df.select(\"text\", \"date\").limit(100000).collect()\n",
    "sentiments = [(x.text, *sentiment(x.text), x.date) for x in tweet_text]\n",
    "\n",
    "sentiment_schema = [\"text\", \"polarity\", \"subjectivity\", \"date\"]\n",
    "sentiments_df = spark.createDataFrame(\n",
    "    data=sentiments,\n",
    "    schema=sentiment_schema\n",
    ")\n",
    "\n",
    "def getSentiment(score):\n",
    "    if score < 0:\n",
    "        return \"negative\"\n",
    "    elif score == 0:\n",
    "        return \"neutral\"\n",
    "    else:\n",
    "        return \"positive\"\n",
    "\n",
    "sentiment_value_udf = udf(lambda x: getSentiment(x), StringType())\n",
    "sentiments_string_df = sentiments_df.withColumn(\"polarity\", sentiment_value_udf(col(\"polarity\")))\n",
    "\n",
    "sentiments_string_df = sentiments_string_df.select('polarity').groupBy('polarity').count()\n",
    "\n",
    "polarity = sentiments_string_df.select(\"polarity\").collect()\n",
    "x = list(map(lambda r: (r['polarity']), polarity))\n",
    "\n",
    "count = sentiments_string_df.select(\"count\").collect()\n",
    "y = list(map(lambda r: int(r['count']), count))\n",
    "\n",
    "plt.subplots(figsize=(16, 8))\n",
    "plt.bar(x,y, color='blue', label='Polarity Frequency')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "max = sentiments_df.select(\"date\").agg({\"date\": \"max\"}).collect()[0][\"max(date)\"]\n",
    "min = sentiments_df.select(\"date\").agg({\"date\": \"min\"}).collect()[0][\"min(date)\"]\n",
    "positive_df = sentiments_df.select(\"date\", \"polarity\").filter(col(\"polarity\") > 0).groupBy(\"date\").count()\n",
    "negative_df = sentiments_df.select(\"date\", \"polarity\").filter(col(\"polarity\") < 0).groupBy(\"date\").count()\n",
    "# neutral_df = sentiments_df.select(\"date\", \"polarity\").filter(col(\"polarity\") == 0).groupBy(\"date\").count()\n",
    "\n",
    "btc_stock = yf.Ticker(\"BTC-USD\")\n",
    "\n",
    "btc_stock = btc_stock.history(start=min, end=max)\n",
    "btc_df = spark.createDataFrame(btc_stock)\n",
    "\n",
    "# Y FOR BTC VOLUME\n",
    "y_rows = btc_df.select(\"Open\").collect()\n",
    "btc_y = list(map(lambda r: float(r['Open']), y_rows))\n",
    "\n",
    "# X FOR POSITIVE SENTIMENT\n",
    "positive_dates = positive_df.select(\"date\").collect()\n",
    "positives_x = list(map(lambda r: (r['date']), positive_dates))\n",
    "\n",
    "# COUNT OF POSITIVE TWEETS\n",
    "positive_y_rows = positive_df.select(\"count\").collect()\n",
    "positive_y = list(map(lambda r: r['count'], positive_y_rows))\n",
    "\n",
    "# COUNT OF NEUTRAL TWEETS\n",
    "# neutral_y_rows = neutral_df.select(\"count\").collect()\n",
    "# neutral_y = list(map(lambda r: r['count'], neutral_y_rows))\n",
    "\n",
    "# COUNT OF NEGATIVE TWEETS\n",
    "negative_y_rows = negative_df.select(\"count\").collect()\n",
    "negative_y = list(map(lambda r: r['count'], negative_y_rows))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "ax.plot(positive_y, color='green', label='Positive Tweet Volume')\n",
    "ax.tick_params(axis='y')\n",
    "\n",
    "ax2 = ax.twinx()\n",
    "ax2.plot(btc_y, color='yellow', label='BTC Value')\n",
    "ax2.tick_params(axis='y')\n",
    "\n",
    "ax3 = ax.twinx()\n",
    "ax3.plot(negative_y, color='red', label='Negative Tweet Volume')\n",
    "ax3.tick_params(axis='y')\n",
    "\n",
    "# ax4 = ax.twinx()\n",
    "# ax4.plot(neutral_y, color='black', label='Neutral Tweet Volume')\n",
    "# ax4.tick_params(axis='y')\n",
    "\n",
    "\n",
    "lines, labels = ax.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "lines3, labels3 = ax3.get_legend_handles_labels()\n",
    "ax2.legend(lines + lines2 + lines3, labels + labels2 + labels3, loc=0)\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "For the sake of not cluttering the chart with futile words such as 'a', 'in' or 'and' we left out the aforementioned ones as well as a short - yet cherry-picked - list of others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "word_occurance = tweets_df.withColumn('word', F.explode(F.split(F.col('text'), ' '))).groupBy('word').count().sort('count', ascending=False).where(\n",
    "    \"word not in (' ', '', 'the', 'a', 'to', 'and', 'a', 'in', 'of', 'for', 'you', 'will', 'be', 'on', 'this', 'i', 'The', 'are', 'at', 'it', 'I')\").limit(100)\n",
    "\n",
    "twitter_mask = np.array(Image.open('twitter.jpeg'))\n",
    "freqs = {r.asDict()['word'] : r.asDict()['count'] for r in word_occurance.collect()}\n",
    "wordcloud = WordCloud(max_font_size=50, max_words=100, background_color=\"white\", mask=twitter_mask).generate_from_frequencies(freqs)\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# hashtags_occurance = tweets_df.withColumn('hashtag', F.explode(col('hashtags'))).groupBy('hashtag').count().sort('count', ascending=False).limit(20)\n",
    "\n",
    "hashtags_occurance = tweets_df.select(\"hashtags\").withColumn(\"hashtag\", F.explode(col('hashtags'))).withColumn(\"hashtag\", F.lower(\"hashtag\")).groupBy('hashtag').count().sort(desc('count')).limit(20)\n",
    "\n",
    "x = get_column_as_list(hashtags_occurance, \"hashtag\")\n",
    "y = get_column_as_list(hashtags_occurance, \"count\")\n",
    "\n",
    "plt.subplots(figsize=(16, 8))\n",
    "plt.barh(x,y, color='orange', label='Volume')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "presentatie\n",
    "---\n",
    "\n",
    "Making of\n",
    "\n",
    "Moeilijkheden.\n",
    "\n",
    "Plezantigheden\n",
    "\n",
    "Beetje commentaar\n",
    "\n",
    "Gene ppt"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "950bcc60cdb4806130dd1b0626e927a3ecaad35b0daba1c028607da664f6fb11"
  },
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow] *",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}